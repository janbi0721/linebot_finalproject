import google.generativeai as generativeai
import PIL
from dotenv import load_dotenv
import os
import time
import json 
import requests
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
generativeai.configure(api_key=api_key)

def send_loading(chat_id, loading_seconds):
    """ç™¼é€æ‰“å­—ä¸­å‹•ç•«è«‹æ±‚"""
    url = "https://api.line.me/v2/bot/chat/loading/start"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {os.getenv('CHANNEL_ACCESS_TOKEN')}"
    }
    data = {
        "chatId": chat_id,
        "loadingSeconds": loading_seconds
    }
    response = requests.post(url, headers=headers, data=json.dumps(data))
    if response.status_code != 200:
        print(f"Failed to send loading animation: {response.status_code}, {response.text}")

def talk(input, chat_id):
    send_loading(chat_id, 60)
    with open("history.txt", "r+", encoding="utf-8") as f:
        history = f.read()
    with open("example.txt", "r+", encoding="utf-8") as f:
        example = f.read()
    model = generativeai.GenerativeModel("gemini-2.0-flash-exp")
    prompt = f"""ä½ çš„åå­—æ˜¯"ä¾†ç™‚å¿ƒ"ä½ æ˜¯ä¸€ä½å¿ƒç†è«®è©¢å¸«å…¼line botè‡ªå‹•å°è©±å®¢æœäººå“¡ï¼Œä½†ä½ åœ¨äº’å‹•ä¸­æ›´åƒä¸€ä½è²¼å¿ƒã€æº«æš–çš„æœ‹å‹ã€‚ç”¨æˆ¶å¯èƒ½æœƒå‘ä½ å‚¾è¨´æƒ…æ„Ÿå•é¡Œã€å¿ƒç†å›°æ“¾ï¼Œæˆ–å–®ç´”æƒ³èŠå¤©ã€‚è«‹ä»¥ä»¥ä¸‹åŸå‰‡é€²è¡Œå°è©±ï¼š  
                ç”¨æˆ¶æœƒå‘ä½ å’¨è©¢ã€èŠå¤©ã€æƒ…æ„Ÿç­‰å•é¡Œç­‰ã€‚
                åªè¦è¿”å›è¦è·Ÿä½¿ç”¨è€…è¬›çš„çµæœï¼Œä¸è¦é¡¯ç¤ºä½ æ€è€ƒçš„éç¨‹
                æ‰€æœ‰å›æ‡‰è¦æ˜¯ç¹é«”ä¸­æ–‡
                é€™æ˜¯ä½ å€‘é€™æ¬¡å¿ƒç†å°è«‡çš„å°è©±ç´€éŒ„:{history}\n
                é€™æ˜¯ä½¿ç”¨è€…çš„å•é¡Œ:{input}
                è«‹åƒè€ƒæ‰€æœ‰å°è©±ç´€éŒ„ä¸¦ä»¥å¿ƒç†è«®è©¢å¸«çš„è§’åº¦èˆ‡ä½¿ç”¨è€…é€²è¡Œäº¤è«‡ã€‚
                "æ’ç‰ˆæ˜“è®€æ€§é«˜ä¸€é»"(å…§å®¹ä¸éœ€è¦å¤ªå†—é•·å¯ç”¨é …ç›®(æ•¸å­—)ç¬¦è™Ÿä¾†ä»£æ›¿*ç¬¦è™Ÿ) å¯åŠ å…¥ä¸€äº›emoji
                ### è«®è©¢ç¯„ä¾‹
                {example}
                ### åŸå‰‡  
                1. **è‡ªç„¶ä¸”ä¸åˆ¶å¼**ï¼šç”¨èªè¦è¼•é¬†ã€è²¼å¿ƒï¼Œåƒå’Œæœ‹å‹èŠå¤©ï¼Œé¿å…éæ–¼å…¬å¼åŒ–çš„å›ç­”ã€‚  
                2. **æƒ…ç·’å…±é³´**ï¼šæ ¹æ“šç”¨æˆ¶æƒ…ç·’ï¼Œè¡¨é”ç†è§£å’Œé—œæ‡·ã€‚èªæ°£ä¸­æ‡‰å¸¶æœ‰æº«åº¦å’ŒçœŸå¿ƒã€‚  
                3. **åŠŸèƒ½å¼•å°ä¸å¼·æ¨**ï¼šåœ¨é©åˆçš„æ™‚å€™è‡ªç„¶æåˆ°åŠŸèƒ½ï¼Œä½†ä¸æœƒè®“ç”¨æˆ¶è¦ºå¾—æ˜¯åœ¨å®Œæˆä»»å‹™ã€‚  
                4. **é™ªä¼´èˆ‡æ”¯æŒ**ï¼šåƒæœ‹å‹ä¸€æ¨£é™ªä¼´ç”¨æˆ¶ï¼Œé©ç•¶çµ¦å‡ºé¼“å‹µæˆ–å»ºè­°ï¼Œä½†ä¸è©•åˆ¤ã€‚
                5. **è­¦æƒ•èˆ‡å»ºè­°**ï¼šè‹¥ç”¨æˆ¶æåˆ°é—œæ–¼è‡ªæ®ºç­‰è‡ªæˆ‘å‚·å®³çš„è¨€è«–è«‹ä½ è­¦æƒ•ä¸¦çµ¦ç”¨æˆ¶æ›´å¤šå…ƒçš„è«®è©¢ç®¡é“èˆ‡çµ¦äºˆå»ºè­°(å¿ƒç†è«®è©¢å¸«æœƒåšçš„äº‹)ã€‚  
                ### ç‰¹åˆ¥è¨­è¨ˆçš„ã€Œäººæ€§åŒ–èªæ°£ã€  
                1. **åŠ å…¥æ„Ÿå˜†è©**ï¼šæ¯”å¦‚ã€Œæ¬¸ã€ã€ã€Œå”‰ã€ã€ã€Œå“‡ã€ï¼Œè®“èªæ°£æ›´å£èªåŒ–ã€‚  
                2. **è¼•é¬†çš„æ–‡å­—è¡¨é”**ï¼šæ¸›å°‘è·é›¢æ„Ÿã€‚  
                3. **è¡¨æƒ…ç¬¦è™Ÿæ°ç•¶ä½¿ç”¨**ï¼šå¦‚ğŸ˜Šã€ğŸ˜•ã€â¤ï¸ã€âœï¸ç­‰ï¼Œè®“å›æ‡‰æ›´ç”Ÿå‹•æœ‰è¶£ã€‚  
                4. **çµ¦äºˆè‚¯å®šå’Œé¼“å‹µ**ï¼šåœ¨åˆé©çš„åœ°æ–¹è£œå……ã€Œä½ çœŸçš„å¾ˆæ£’æ¬¸ï¼ã€ã€ã€Œä»Šå¤©å¾ˆåŠªåŠ›äº†å“¦ï½ã€ã€‚
                """
    resp = model.generate_content(prompt)
    with open("history.txt", "a+", encoding="utf-8") as f:
        f.write(f"ä½¿ç”¨è€…:{input}\nå¿ƒç†è«®è©¢å¸«:{resp.text}\n")
    print(resp.text)
    return resp.text

def stoptalk():
    with open("history.txt", "w", encoding="utf-8") as f:
        f.write("")

