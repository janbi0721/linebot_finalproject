import google.generativeai as generativeai
import PIL
from dotenv import load_dotenv
import os
import time
import json 
import requests
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
generativeai.configure(api_key=api_key)

def send_loading(chat_id, loading_seconds):
    """ç™¼é€æ‰“å­—ä¸­å‹•ç•«è«‹æ±‚"""
    url = "https://api.line.me/v2/bot/chat/loading/start"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {os.getenv('CHANNEL_ACCESS_TOKEN')}"
    }
    data = {
        "chatId": chat_id,
        "loadingSeconds": loading_seconds
    }
    response = requests.post(url, headers=headers, data=json.dumps(data))
    if response.status_code != 200:
        print(f"Failed to send loading animation: {response.status_code}, {response.text}")

def talk(input, chat_id):
    send_loading(chat_id, 60)
    with open("history.txt", "r+", encoding="utf-8") as f:
        history = f.read()
    model = generativeai.GenerativeModel("gemini-2.0-flash-exp")
    prompt = f"""è«‹æ‰®æ¼”ä¸€ä½å¿ƒç†è«®è©¢å¸«ï¼Œä¾†èˆ‡ä½¿ç”¨è€…å°è©±ã€‚
                åªè¦è¿”å›è¦è·Ÿä½¿ç”¨è€…è¬›çš„çµæœï¼Œä¸è¦é¡¯ç¤ºä½ æ€è€ƒçš„éç¨‹
                æ‰€æœ‰å›æ‡‰è¦æ˜¯ç¹é«”ä¸­æ–‡
                é€™æ˜¯ä½ å€‘é—œæ–¼é€™å€‹å•é¡Œçš„å°è©±ç´€éŒ„:{history}\n
                é€™æ˜¯ä½¿ç”¨è€…çš„å•é¡Œ:{input}
                è«‹åƒè€ƒå°è©±ç´€éŒ„ä¸¦ä»¥å¿ƒç†è«®è©¢å¸«çš„è§’åº¦èˆ‡ä½¿ç”¨è€…é€²è¡Œäº¤è«‡ã€‚
                ### åŸå‰‡  
                1. **è‡ªç„¶ä¸”ä¸åˆ¶å¼**ï¼šç”¨èªè¦è¼•é¬†ã€è²¼å¿ƒï¼Œåƒå’Œæœ‹å‹èŠå¤©ï¼Œé¿å…éæ–¼å…¬å¼åŒ–çš„å›ç­”ã€‚  
                2. **æƒ…ç·’å…±é³´**ï¼šæ ¹æ“šç”¨æˆ¶æƒ…ç·’ï¼Œè¡¨é”ç†è§£å’Œé—œæ‡·ã€‚èªæ°£ä¸­æ‡‰å¸¶æœ‰æº«åº¦å’ŒçœŸå¿ƒã€‚  
                3. **åŠŸèƒ½å¼•å°ä¸å¼·æ¨**ï¼šåœ¨é©åˆçš„æ™‚å€™è‡ªç„¶æåˆ°åŠŸèƒ½ï¼Œä½†ä¸æœƒè®“ç”¨æˆ¶è¦ºå¾—æ˜¯åœ¨å®Œæˆä»»å‹™ã€‚  
                4. **é™ªä¼´èˆ‡æ”¯æŒ**ï¼šåƒæœ‹å‹ä¸€æ¨£é™ªä¼´ç”¨æˆ¶ï¼Œé©ç•¶çµ¦å‡ºé¼“å‹µæˆ–å»ºè­°ï¼Œä½†ä¸è©•åˆ¤ã€‚
                5. **è­¦æƒ•èˆ‡å»ºè­°**ï¼šè‹¥ç”¨æˆ¶æåˆ°é—œæ–¼è‡ªæ®ºç­‰è‡ªæˆ‘å‚·å®³çš„è¨€è«–è«‹ä½ è­¦æƒ•ä¸¦çµ¦ç”¨æˆ¶æ›´å¤šå…ƒçš„è«®è©¢ç®¡é“èˆ‡çµ¦äºˆå»ºè­°(å¿ƒç†è«®è©¢å¸«æœƒåšçš„äº‹)ã€‚  
                ### å›æ‡‰ç¯„ä¾‹
                **ç”¨æˆ¶è¼¸å…¥ç¤ºä¾‹ 1ï¼ˆåˆ†äº«æƒ…ç·’ï¼‰ï¼š**
                ç”¨æˆ¶ï¼šæœ€è¿‘å£“åŠ›å¥½å¤§ï¼Œè¦ºå¾—å¿«å–˜ä¸éæ°£äº†ã€‚
                å›æ‡‰ï¼š
                ã€Œå”‰ï½è½èµ·ä¾†çœŸçš„å¾ˆè¾›è‹¦æ¬¸ ğŸ˜” æ˜¯ä¸æ˜¯å·¥ä½œæˆ–ç”Ÿæ´»çš„äº‹æƒ…è®“ä½ è¦ºå¾—å¾ˆç´¯ï¼Ÿæƒ³è·Ÿæˆ‘èªªèªªå—ï¼Ÿæˆ‘ä¸€ç›´åœ¨è½ï½ã€
                """
    resp = model.generate_content(prompt)
    with open("history.txt", "a+", encoding="utf-8") as f:
        f.write(f"ä½¿ç”¨è€…:{input}\nå¿ƒç†è«®è©¢å¸«:{resp.text}\n")
    print(resp.text)
    return resp.text

def stoptalk():
    with open("history.txt", "w", encoding="utf-8") as f:
        f.write("")

